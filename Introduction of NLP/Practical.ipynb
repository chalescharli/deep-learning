{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Implementation of preprocessing of Text with NLTK (Tokenization, Stemming, Lemmatization and removal of stop words \n",
    "in NLP)\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In NLP, preprocessing text involves several steps to clean and prepare the text data for analysis. Here's how you can implement tokenization, stemming, lemmatization, and stop word removal using NLTK (Natural Language Toolkit) in Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization\n",
    "\n",
    "-------\n",
    "\n",
    "Tokenization breaks text into smaller units like words or sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words:\n",
      "['Tokenization', 'breaks', 'text', 'into', 'smaller', 'units', 'like', 'words', 'or', 'sentences']\n",
      "\n",
      "Tokenized Sentences:\n",
      "['Tokenization breaks text into smaller units like words or sentences']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "text = \"Tokenization breaks text into smaller units like words or sentences\"\n",
    "\n",
    "\n",
    "words = word_tokenize(text)\n",
    "print(\"Tokenized Words:\")\n",
    "print(words)\n",
    "\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"\\nTokenized Sentences:\")\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Stemming\n",
    "\n",
    "-------------\n",
    "Cutting words down to their root form, like changing \"running\" to \"run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words:\n",
      "['play', 'play', 'play']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stemming example\n",
    "words = [\"playing\", \"played\", \"plays\"]\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"Stemmed Words:\")\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Lemmatization:\n",
    "\n",
    "-----------------\n",
    " Similar to stemming, but changes words to their dictionary form, like \"better\" to \"good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words:\n",
      "['playing', 'played', 'play']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatization example\n",
    "words = [\"playing\", \"played\", \"plays\"]\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Lemmatized Words:\")\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Removal of Stop Words\n",
    "\n",
    "----------\n",
    "Stop words are common words  that often do not contribute to the meaning of the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words after Stopword Removal:\n",
      "['example', 'sentence', 'demonstrating', 'stop', 'word', 'removal', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rr749\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Example text\n",
    "text = \"This is an example sentence demonstrating stop word removal.\"\n",
    "\n",
    "# Remove stop words\n",
    "filtered_words = [word for word in word_tokenize(text) if word.lower() not in stop_words]\n",
    "print(\"Filtered Words after Stopword Removal:\")\n",
    "print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "---------------------------\n",
    "- Tokenization: Splits text into words or sentences.\n",
    "\n",
    "- Stemming: Reduces words to their root form.\n",
    "\n",
    "- Lemmatization: Converts words to their base or dictionary form.\n",
    "\n",
    "- Stop Word Removal: Filters out common words that do not carry much meaning.\n",
    "\n",
    "- These preprocessing steps are essential for preparing text data before applying more advanced NLP techniques such as sentiment analysis, text classification, or information retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
